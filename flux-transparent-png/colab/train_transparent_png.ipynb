{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/your-username/flux-transparent-png/blob/main/colab/train_transparent_png.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Flux.1 Transparent PNG Training\n",
        "\n",
        "This notebook allows you to train a modified VAE on transparent PNG images using Flux.1 dev. The trained VAE can then be used to generate new transparent PNG images without backgrounds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "First, let's install the required dependencies and clone the repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Check if running in Google Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(f\"Running in Colab: {IN_COLAB}\")\n",
        "\n",
        "# Mount Google Drive if in Colab\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted at /content/drive\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/your-username/flux-transparent-png.git\n",
        "%cd flux-transparent-png/python"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Install dependencies\n",
        "!pip install torch torchvision diffusers transformers pillow numpy matplotlib tqdm"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Set up the training configuration parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Configuration parameters\n",
        "DATA_DIR = \"/content/drive/MyDrive/SD-Data/TrainData/4000_PNG/TEST\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/VAE-DECODER\"\n",
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/VAE-DECODER/checkpoints\"\n",
        "BATCH_SIZE = 8\n",
        "NUM_EPOCHS = 100\n",
        "LEARNING_RATE = 1e-4\n",
        "ALPHA_WEIGHT = 2.0\n",
        "IMAGE_SIZE = 512\n",
        "\n",
        "# Create output directories\n",
        "!mkdir -p {OUTPUT_DIR}\n",
        "!mkdir -p {CHECKPOINT_DIR}"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explore Training Data\n",
        "\n",
        "Let's explore the training data to make sure it's properly loaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Import the dataset class\n",
        "from train_transparent_png import TransparentPNGDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Create dataset\n",
        "dataset = TransparentPNGDataset(DATA_DIR, image_size=IMAGE_SIZE)\n",
        "print(f\"Found {len(dataset)} images in {DATA_DIR}\")\n",
        "\n",
        "# Create dataloader\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# Visualize a few samples\n",
        "fig, axes = plt.subplots(3, 2, figsize=(12, 18))\n",
        "\n",
        "for i, batch in enumerate(dataloader):\n",
        "    if i >= 3:\n",
        "        break\n",
        "        \n",
        "    # Get image\n",
        "    image = batch['image'][0]  # Remove batch dimension\n",
        "    \n",
        "    # Convert to numpy for visualization\n",
        "    image_np = image.numpy()\n",
        "    \n",
        "    # Scale from [-1, 1] to [0, 1]\n",
        "    image_np = (image_np + 1) / 2\n",
        "    \n",
        "    # Clip to valid range\n",
        "    image_np = np.clip(image_np, 0, 1)\n",
        "    \n",
        "    # RGB channels\n",
        "    axes[i, 0].imshow(np.transpose(image_np[:3], (1, 2, 0)))\n",
        "    axes[i, 0].set_title(f\"Sample {i+1} - RGB Channels\")\n",
        "    axes[i, 0].axis('off')\n",
        "    \n",
        "    # Alpha channel\n",
        "    axes[i, 1].imshow(image_np[3], cmap='gray')\n",
        "    axes[i, 1].set_title(f\"Sample {i+1} - Alpha Channel\")\n",
        "    axes[i, 1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the Model\n",
        "\n",
        "Now let's train the transparent VAE model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Run the training\n",
        "!python train_transparent_png.py \\\n",
        "  --data_dir=\"{DATA_DIR}\" \\\n",
        "  --output_dir=\"{OUTPUT_DIR}\" \\\n",
        "  --checkpoint_dir=\"{CHECKPOINT_DIR}\" \\\n",
        "  --batch_size={BATCH_SIZE} \\\n",
        "  --num_epochs={NUM_EPOCHS} \\\n",
        "  --learning_rate={LEARNING_RATE} \\\n",
        "  --alpha_weight={ALPHA_WEIGHT} \\\n",
        "  --image_size={IMAGE_SIZE}"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save the Trained Models\n",
        "\n",
        "After training, let's save the VAE and decoder models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Save the models\n",
        "!python save_vae_decoder.py \\\n",
        "  --checkpoint_dir=\"{CHECKPOINT_DIR}\" \\\n",
        "  --output_dir=\"{OUTPUT_DIR}\" \\\n",
        "  --vae_filename=\"transparent_vae.pt\" \\\n",
        "  --decoder_filename=\"transparent_decoder.pt\" \\\n",
        "  --verify"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Training Results\n",
        "\n",
        "Let's visualize some of the training results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "import glob\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Find visualization files\n",
        "vis_dirs = sorted(glob.glob(f\"{OUTPUT_DIR}/visualizations_epoch_*\"))\n",
        "if vis_dirs:\n",
        "    latest_vis_dir = vis_dirs[-1]\n",
        "    print(f\"Showing visualizations from {latest_vis_dir}\")\n",
        "    \n",
        "    # Get original and reconstructed images\n",
        "    original_files = sorted(glob.glob(f\"{latest_vis_dir}/original_*.png\"))\n",
        "    recon_files = sorted(glob.glob(f\"{latest_vis_dir}/reconstructed_*.png\"))\n",
        "    \n",
        "    # Display images\n",
        "    n = min(3, len(original_files))\n",
        "    fig, axes = plt.subplots(n, 2, figsize=(12, 6*n))\n",
        "    \n",
        "    for i in range(n):\n",
        "        # Original\n",
        "        original = Image.open(original_files[i])\n",
        "        axes[i, 0].imshow(original)\n",
        "        axes[i, 0].set_title(f\"Original {i+1}\")\n",
        "        axes[i, 0].axis('off')\n",
        "        \n",
        "        # Reconstructed\n",
        "        recon = Image.open(recon_files[i])\n",
        "        axes[i, 1].imshow(recon)\n",
        "        axes[i, 1].set_title(f\"Reconstructed {i+1}\")\n",
        "        axes[i, 1].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No visualization files found.\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "Now that you have trained the VAE and decoder models, you can use them to generate transparent PNG images. See the `generate_transparent_png.ipynb` notebook for details on how to generate images using your trained models."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "train_transparent_png.ipynb",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
